# -*- coding: utf-8 -*-
"""Part 5: Training Loop

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t8ZEcaU1EW8aLCJjcsF6ib5AxjN3Ol8h
"""

# Training
model = MultiModalFakeNewsModel().to(device)
dataset = MultiModalDataset(df)
dataloader = DataLoader(dataset, batch_size=4, shuffle=True)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

print("Starting training...")
for epoch in range(2):
    model.train()
    total_loss = 0
    for batch in dataloader:
        input_ids, attn_mask, img, vid, label = [x.to(device) for x in batch]
        optimizer.zero_grad()
        output = model(input_ids, attn_mask, img, vid)
        loss = criterion(output, label)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}: Loss = {total_loss/len(dataloader):.4f}")

# Save model
torch.save(model.state_dict(), "best_model.pt")

# Evaluation
model.eval()
y_true, y_pred = [], []
with torch.no_grad():
    for batch in dataloader:
        input_ids, attn_mask, img, vid, label = [x.to(device) for x in batch]
        logits = model(input_ids, attn_mask, img, vid)
        predictions = torch.argmax(logits, dim=1)
        y_true.extend(label.cpu().numpy())
        y_pred.extend(predictions.cpu().numpy())
print("\nClassification Report:")
print(classification_report(y_true, y_pred))